name: CI/CD Pipeline

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  test:
    name: Test and Lint
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: |
          uv sync --all-extras

      - name: Run ruff linter
        run: |
          uv run ruff check .

      - name: Run ruff formatter check
        run: |
          uv run ruff format --check .

      - name: Run tests
        env:
          # Убеждаемся, что Kaggle API не используется в тестах
          USE_KAGGLE_DATA: "false"
          KAGGLE_API_TOKEN: ""
        run: |
          uv run pytest --cov=bankshield --cov-report=xml --cov-report=term

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: ./coverage.xml
          fail_ci_if_error: false

  deploy:
    name: Deploy to Server
    runs-on: ubuntu-latest
    needs: test
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Нужно для сравнения коммитов

      - name: Check changed files
        id: changed-files
        run: |
          if [ "${{ github.event_name }}" == "push" ]; then
            # Получаем список измененных файлов
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD || echo "")
            echo "Changed files:"
            echo "$CHANGED_FILES"

            if [ -z "$CHANGED_FILES" ]; then
              echo "no_changes=true" >> $GITHUB_OUTPUT
              echo "only_dags=false" >> $GITHUB_OUTPUT
              echo "deploy_containers=" >> $GITHUB_OUTPUT
              echo "No files changed, skipping deployment"
              exit 0
            fi

            # Проверяем, изменились ли только файлы в airflow/dags/
            ONLY_DAGS=$(echo "$CHANGED_FILES" | grep -v "^airflow/dags/" | grep -v "^$" || true)

            if [ -z "$ONLY_DAGS" ] && [ -n "$CHANGED_FILES" ]; then
              echo "no_changes=false" >> $GITHUB_OUTPUT
              echo "only_dags=true" >> $GITHUB_OUTPUT
              echo "deploy_containers=" >> $GITHUB_OUTPUT
              echo "Only DAG files changed, skipping container deployment"
              exit 0
            fi

            # Определяем, какие контейнеры нужно передеплоить
            CONTAINERS=""

            # Docker compose или Dockerfile - все контейнеры
            if echo "$CHANGED_FILES" | grep -qE "(docker-compose\.yml|Dockerfile|Dockerfile\.api)"; then
              CONTAINERS="all"
            else
              # data-collector и api контейнеры
              if echo "$CHANGED_FILES" | grep -qE "^src/"; then
                CONTAINERS="${CONTAINERS} data-collector api"
              fi

              # Airflow контейнеры
              if echo "$CHANGED_FILES" | grep -qE "^airflow/(config|plugins)/"; then
                CONTAINERS="${CONTAINERS} airflow-webserver airflow-scheduler"
              fi

              # DBT изменения - Airflow и Elementary
              if echo "$CHANGED_FILES" | grep -qE "^dbt/"; then
                CONTAINERS="${CONTAINERS} airflow-webserver airflow-scheduler elementary"
              fi

              # Grafana
              if echo "$CHANGED_FILES" | grep -qE "^grafana/"; then
                CONTAINERS="${CONTAINERS} grafana"
              fi

              # PostgreSQL или MongoDB - обычно не требуют передеплоя, но можно добавить
              if echo "$CHANGED_FILES" | grep -qE "(postgres|mongodb)"; then
                CONTAINERS="${CONTAINERS} postgres mongodb"
              fi
            fi

            # Убираем дубликаты и лишние пробелы
            CONTAINERS=$(echo "$CONTAINERS" | tr ' ' '\n' | sort -u | tr '\n' ' ' | xargs)

            echo "no_changes=false" >> $GITHUB_OUTPUT
            echo "only_dags=false" >> $GITHUB_OUTPUT
            echo "deploy_containers=$CONTAINERS" >> $GITHUB_OUTPUT
            echo "Containers to deploy: $CONTAINERS"
          else
            echo "no_changes=false" >> $GITHUB_OUTPUT
            echo "only_dags=false" >> $GITHUB_OUTPUT
            echo "deploy_containers=all" >> $GITHUB_OUTPUT
            echo "Not a push event, full deployment"
          fi

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts

      - name: Skip deployment
        if: steps.changed-files.outputs.no_changes == 'true'
        run: |
          echo "No changes detected, skipping deployment"

      - name: Deploy only DAGs
        if: steps.changed-files.outputs.only_dags == 'true'
        env:
          SERVER_HOST: ${{ secrets.SERVER_HOST }}
          SERVER_USER: ${{ secrets.SERVER_USER }}
          DEPLOY_PATH: ${{ secrets.DEPLOY_PATH }}
        run: |
          echo "Deploying only DAG files..."
          rsync -avz \
            airflow/dags/ $SERVER_USER@$SERVER_HOST:$DEPLOY_PATH/airflow/dags/
          echo "DAG files deployed successfully"

      - name: Deploy to server
        if: steps.changed-files.outputs.only_dags == 'false' && steps.changed-files.outputs.no_changes == 'false'
        env:
          SERVER_HOST: ${{ secrets.SERVER_HOST }}
          SERVER_USER: ${{ secrets.SERVER_USER }}
          DEPLOY_PATH: ${{ secrets.DEPLOY_PATH }}
        run: |
          # Create deployment directory if it doesn't exist
          ssh $SERVER_USER@$SERVER_HOST "mkdir -p $DEPLOY_PATH"

          # Copy files to server
          rsync -avz --delete \
            --exclude '.git' \
            --exclude '__pycache__' \
            --exclude '*.pyc' \
            --exclude '.pytest_cache' \
            --exclude 'htmlcov' \
            --exclude '.venv' \
            --exclude 'data' \
            --exclude 'airflow/logs' \
            --exclude 'dbt/dbt_packages' \
            --exclude 'dbt/target' \
            --exclude 'dbt/logs' \
            --exclude 'dbt/edr_target' \
            ./ $SERVER_USER@$SERVER_HOST:$DEPLOY_PATH/

          # Set permissions for airflow, data, and dbt directories
          ssh $SERVER_USER@$SERVER_HOST "cd $DEPLOY_PATH && \
            mkdir -p airflow/logs airflow/config airflow/plugins data dbt/target dbt/dbt_packages && \
            sudo chown -R \$USER:\$USER airflow data dbt 2>/dev/null || chown -R \$USER:\$USER airflow data dbt 2>/dev/null || true && \
            sudo chmod -R 777 airflow data dbt 2>/dev/null || chmod -R 777 airflow data dbt 2>/dev/null || true"

          # Deploy on server
          CONTAINERS="${{ steps.changed-files.outputs.deploy_containers }}"
          if [ "$CONTAINERS" = "all" ] || [ -z "$CONTAINERS" ]; then
            echo "Deploying all containers..."
            ssh $SERVER_USER@$SERVER_HOST "cd $DEPLOY_PATH && \
              docker compose down || true && \
              docker compose pull || true && \
              docker compose up -d --build && \
              sleep 30 && \
              docker compose ps && \
              docker compose logs --tail=50"
          else
            echo "Deploying specific containers: $CONTAINERS"
            ssh $SERVER_USER@$SERVER_HOST "cd $DEPLOY_PATH && \
              for container in $CONTAINERS; do
                echo \"Rebuilding container: \$container\"
                docker compose stop \$container || true
                docker compose rm -f \$container || true
                docker compose build \$container || true
                docker compose up -d \$container || true
              done && \
              sleep 10 && \
              docker compose ps && \
              docker compose logs --tail=50 $CONTAINERS"
          fi

      - name: Health check
        env:
          SERVER_HOST: ${{ secrets.SERVER_HOST }}
        run: |
          echo "Waiting for services to be ready..."
          sleep 10

          # Check if services are responding
          if curl -f http://$SERVER_HOST:8080/health || true; then
            echo "Airflow is up"
          fi

          if curl -f http://$SERVER_HOST:3001/api/health || true; then
            echo "Grafana is up"
          fi
